---
layout: post
title: Mathematical Explanation â€“ Variational Auto-Encoder
---

This article was created for those who would like to understand more about the mathematical reasoning behind a model such as VAE. For this purpose I am introducing a range of different concepts in statistics
that help understand the decision making of this paper's  authors. I marked them with (in short) to indicate that they are only a short summery of relevant mathematical statements for this post.
If you have any questions or suggestions  regarding this article feel free to leave a comment.

# Table of Contents
1. [Required knowledge](#required-knowledge)
2. [Introduction](#introduction)
3. [Third Example](#third-example)
4. [Fourth Example](#fourth-examplehttpwwwfourthexamplecom)


## Required knowledge

The following topics are necessary to understand this article:
- Basic probability theory
- Basic statistics
- Auto-Encoder

I will recap some necessary core concepts from the following topics:
- MLE, MAP estimator & EM
- Bayesian inference
- Variational inference
- Monte Carlo Estimate

If you are keen to know more details about each topic I can suggest

## Introduction

Applications of such a model is amongst others data representation, generating artificial data and image super-resolution.
As we know from a standard AE an encoder tries to find the most relevant features of observed data and captures this information in a feature vector and the decoder has to reconstruct the original data. Hence, 
the architecture of a standard AE can be used for data representation. If we sample some features from a latent variable and feed that to the decoder we could use the decoder  for generating artificial data.
Those characteristics make an AE a useful basic structure for these goals.

Here is an explanatory illustration that shows the model design:

![_config.yml]({{ site.baseurl }}/images/vae_arch.png)

In a VAE, the encoder's task is to describe the distributions of those features based on the data we observed. Whereas during inference, just the decoder will be used as a generator, during training, 
the entire model will be treated as a generator model. From that perspective, sampled values from the latent variable are generated from an unknown prior distribution. Hence, we would like to construct a model based on that perspective.

***The goal of this model is to find out from which distribution family these latent variables are drawn. In addition, we would like to increase the likelihood that such a latent distribution originated from our observed data.***

Throughout this article I assume that the data is iid. by some random process.

Let's first formulate that mathematically:

The encoder can be viewed as the posterior $$p_{\hat{\theta}}(\bold{z} | \bold{x})$$ whereas the decoder as the likelihood $$p_{\hat{\theta}}(\bold{x} | \bold{z})$$.
So our model has to find the true parameter within the parameter space $$\Theta$$. For each parameter we get a new density, then the parametric family of densities is the set of all such densities:

\begin{align}
    \mathcal{F}_{p_{\bold{x}}} := \{ p_{\theta}(\bold{x})\ |\ \forall \ \theta \in \Theta \}\\ \nonumber
\end{align}
where $$\bold{x}$$ is a random variable. 

Then our model, a generator, can be expressed as the joint probability:

\begin{align}
    p_{\hat{\theta}}(\bold{x}, \bold{z}) = p_{\hat{\theta}}(\bold{x}| \bold{z})p_{\hat{\theta}}(\bold{z})
\end{align}


## Third Example
## [Fourth Example](http://www.fourthexample.com) 

![_config.yml]({{ site.baseurl }}/images/Screenshot from 2023-07-04 15-50-05.png)
