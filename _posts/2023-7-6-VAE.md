---
layout: post
title: Mathematical Explanation<br/>
       Variational Auto-Encoder
intro: This article was created for those who would like to understand more about the mathematical reasoning behind a model such as VAE. For this purpose I am introducing a range of different concepts ...

start: This article was created for those who would like to understand more about the mathematical reasoning behind a model such as VAE. For this purpose I am introducing a range of different concepts in statistics that help understand the decision-making of this paper's authors. I sum- marized only the most relevant mathematical statements for this post of each topic. To make a VAE more comprehensive I emphasized 
  on the modeling of it and core concepts to computationally realize it. Aditionally, I used day-to-day examples and visualisations that help building an intuition. If you have any questions or suggestions  regarding this topic feel free to contact me.

chapters:   ["Introduction",
             "Modeling",
            "MLE, MAP estimation and EM",
            "Bayesian inference",
             "Variational inference",
            "The Evidence Lower Bound",
            "Monte Carlo Estimate",
            "Reparameterization Trick",
            "SGVB estimator and AEVB algorithm",
            "Sources"]
---

<br/>

### Required knowledge

The following topics are necessary to understand this article:
- Basic probability theory
- Basic statistics

I will recap some necessary core concepts from the following topics:
- MLE, MAP estimator & EM
- Bayesian inference
- Variational inference
- Monte Carlo Estimate

If you are keen to know more details about each topic I can suggest: 
- [Introduction to Mathematical Statistics by Hogg](https://minerva.it.manchester.ac.uk/~saralees/statbook2.pdf) 
- [Mathematical Statistics by van de Geer](https://www.stat.math.ethz.ch/~geer/mathstat.pdf) 
- [Monte Carlo Methods by Kroese](https://people.smp.uq.edu.au/DirkKroese/mccourse.pdf)

<br/>

![_config.yml]({{ site.baseurl }}/images/separator.png)

{% include chapter.html name="Introduction" %}

To understand the architecture of a VAE, a generator, and its mathematical model, we will need to determine first the goal of such a model type and its 
relationship to a discriminator. <br/>
With a discriminative model we want to make a prediction about an attribute based on various observations. Such a model tries to learn the mapping from that data 
to this attribute. To do so it tries to find features of these observations that has the highest correlation with that attribute.
But correlation is not causation. Imagine, when I was a little child, I always got upset when my mom bought apples that had some brown spots.
She always answered: "No, they are good! Those are just some injuries." 
Sure, as if apples are like humans and could have "injuries" - you are just trying to make these cheap apples palatable, was my response back then.
Indeed, my mom was right, injuries of the apples skin exposes its tissue to oxygen that turns polyphenol (a micronutrient in the apple) into melanin. 

Coming back to our problem, we could collect observations, like skin color of an apple and its surface structure and so on, to make a prediction whether an apple is rotten or not.
And, even though my mom was right, brown spots are usually a good indication since some infecting spores of fungi that occur in rotten apples create a brown coloration.
A discriminator would be able to find that correlation, like me as a child. 

<br/> Unfortunately, such a model will eliminate also apples that are healthy to consume but just had an injury. You could argue now, that the correlation 
between the browning of apples and its decay cannot be that high compared to the correlation between the wrinkling of skin and the level of decay.
So we simply take the latter feature to make such a prediction. Though, if we have a dataset where every brown apple is also rotten we couldn't 
differentiate between the relevance of those two features. And typically we choose to build such models to make predictions that we cannot do by our-self, hence, in a realistic problem 
we wouldn't have that knowledge.
Because of those instances, a model that can learn the causal relationship is of course more precise. 
To achieve that it would have to learn the generating process of rotten apples. 

If we abstract this case, we could say that an attribute like the level of decay is just another feature that describes an object.
From that perspective every object is a state of a collection of features or call it attributes if you like.

That would mean that an example of an apple is an expression of the following states:
- mainly red colored
- brown spots
- smooth skin
- *decomposed*
- etc.

Then a trained generator would tell us that the occurrence of all these features' states at the same time is very unrealistic.
In other words, a generator should uncover **how all the features that describe an object are jointly distributed**.
<br/>

Another important goal of training a generator is data completion. For instance, image resolution. For this purpose we need to transform the generator in a conditional generator.
It generates data depending on the state of some additional data, here a low-resolution image.

<br/>

In summary, such a model should be used for semi-supervised learning, data completion and generating realistic data. And it does so by learning
the joint distribution of all features.

![_config.yml]({{ site.baseurl }}/images/separator.png)

<br/>

{% include chapter.html name="Modeling" %}

Some of you may know that finding a proper data representation, that includes selecting the most relevant features of the data, will increase 
the chances of finding a good discriminator. Typical methods include the use an auto-encoder or principal component analysis which tries to find features that 
have no correlation with one another since such a representation is more expressive.



{% include image.html url="/images/2023-7-6-VAE/vae_arch.png" text="An explanatory illustration that shows the model design." %}

<br/>

In a VAE, the encoder's task is to describe the distributions of those features based on the data we observed. Whereas during inference, just the decoder will be used as a generator, during training, 
the entire model will be treated as a generator model. From that perspective, sampled values from the latent variable are generated from an unknown prior distribution. Hence, we would like to construct a model based on that perspective.

*The goal of this model is to find out from which distribution family these latent variables are drawn. In addition, we would like to increase the likelihood that such a latent distribution originated from our observed data.*

Let's first formulate that mathematically:

![_config.yml]({{ site.baseurl }}/images/2023-7-6-VAE/vae_v1.png)

As a directed graph we could view it in the following way: (I made the arc representing p(x) dotted because we will later see that it is intractible.)

Applying this formulation on our model we obtain the following assignment:

![_config.yml]({{ site.baseurl }}/images/2023-7-6-VAE/vae_v1_1.png)

To visualize that concept I created an illustration:

{% include image.html url="/images/2023-7-6-VAE/diag1.png" text="This diagram illustrates assignments of distributions." %}

<br/>

![_config.yml]({{ site.baseurl }}/images/separator.png)

<br/>

{% include chapter.html name="MLE, MAP estimation and EM" %}

<br/>

A typical approach would be to use Maximum Likelihood Estimation (MLE) or Maximum A Posterior Estimation (MAP) to infer the best parameter for our model.

![_config.yml]({{ site.baseurl }}/images/2023-7-6-VAE/vae_v2.png)

![_config.yml]({{ site.baseurl }}/images/2023-7-6-VAE/vae_v3.png)

<br/>

![_config.yml]({{ site.baseurl }}/images/separator.png)

<br/>

{% include chapter.html name="Bayesian inference" %}

<br/>

Next, we want to find the posterior ``p(z|x)```.  Bayesian inference is a method in which Baye's theorem is used to update the probability for a hypothesis as more evidence becomes available. Here, we view z as the hypothesis.

![_config.yml]({{ site.baseurl }}/images/2023-7-6-VAE/vae_v5.png)

So computing the true posterior requires approximate inference but methods like Monte Carlo EM are not useful for our case since it will be too slow for complex models with large datasets.

<br/>

![_config.yml]({{ site.baseurl }}/images/separator.png)

<br/>

{% include chapter.html name="Variational inference" %}

<br/>

Instead, variational inference will bring us closer to the solution. It is a method that approximates probability density functions through optimization. The basic approach is to create a family of possible densities and with a distance measure to find the density from this family that is the closest to the one we want to approximate.

![_config.yml]({{ site.baseurl }}/images/2023-7-6-VAE/vae_v6.png)

<br/>

Equation (8) is by itself not computable in our case because it involves calculating log(p(x)). The following calculation will show that:

![_config.yml]({{ site.baseurl }}/images/2023-7-6-VAE/vae_v7.png)

As we can see in equation (6), that optimization problem contains the term p(x) which makes equation (8) not computable.

<br/>

![_config.yml]({{ site.baseurl }}/images/separator.png)

<br/>

{% include chapter.html name="The Evidence Lower Bound" %}

<br/>

Even though equation (8) is not computable we can use it after applying some tweaks.

![_config.yml]({{ site.baseurl }}/images/2023-7-6-VAE/vae_v8.png)

With some further transformations we can rewrite that function to obtain our objective:

![_config.yml]({{ site.baseurl }}/images/2023-7-6-VAE/vae_v9.png)

The name originates from the following property:

![_config.yml]({{ site.baseurl }}/images/2023-7-6-VAE/vae_v10.png)

Below you can find an animated example where p is a normal density function and q a gamma density function. 
The plot on top is the graph of the ELBO and below a plot of p and q with varying values for its parameters.
Taking the derivative of the ELBO w.r.t. to the parameter of q we see that zero is the maximum value. 

<br/> 

![_config.yml]({{ site.baseurl }}/images/2023-7-6-VAE/animation.gif)

![_config.yml]({{ site.baseurl }}/images/separator.png)

<br/>


{% include chapter.html name="Reparameterization Trick" %}

linear combination of RV: pi_1, ..., pi_n : pi_1 * P(X1) + ... + pi_n * P(X2)
mixture model but then all have differnt parameters


from script : VI where each data-case has a separate variational distribu-
tion, which is inefficient for large data-sets. The recognition model uses
one set of parameters to model the relation between input and latent
variables and as such is called “amortized inference”.

<br/>

{% include chapter.html name="Sources" %}

- [Image Super-Resolution With Deep Variational Autoencoders by Darius Chira, Ilian Haralampiev](https://arxiv.org/pdf/2203.09445.pdf)
- [Auto-Encoding Variational Bayes by Diederik P. Kingma, Max Welling](https://arxiv.org/pdf/1312.6114.pdf)
- [An Introduction to Variational Autoencoders by Diederik P. Kingma, Max Welling](https://arxiv.org/pdf/1906.02691.pdf)


![_config.yml]({{ site.baseurl }}/images/separator.png)

