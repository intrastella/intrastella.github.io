---
layout: post
title: Mathematical Explanation 
       Variational Auto-Encoder
intro: This article was created for those who would like to understand more about the mathematical reasoning behind a model such as VAE. For this purpose I am introducing a range of different concepts ...

start: This article was created for those who would like to understand more about the mathematical reasoning behind a model such as VAE. For this purpose I am introducing a range of different concepts in statistics that help understand the decision-making of this paper's  authors. I marked them with 'recap' to indicate that they are only a short summery of relevant mathematical statements for this post. If you have any questions or suggestions  regarding this article feel free to contact me.

chapters:   ["Introduction",
            "MLE, MAP estimation and EM",
            "Bayesian inference",
             "Variational inference",
            "The Evidence Lower Bound",
            "Monte Carlo Estimate",
            "Reparametrization Trick",
            "SGVB estimator and AEVB algorithm",
            "Sources"]
---

<br/>

### Required knowledge

The following topics are necessary to understand this article:
- Basic probability theory
- Basic statistics
- Auto-Encoder

I will recap some necessary core concepts from the following topics:
- MLE, MAP estimator & EM
- Bayesian inference
- Variational inference
- Monte Carlo Estimate

If you are keen to know more details about each topic I can suggest: 
- [Introduction to Mathematical Statistics by Hogg](https://minerva.it.manchester.ac.uk/~saralees/statbook2.pdf) 
- [Mathematical Statistics by van de Geer](https://www.stat.math.ethz.ch/~geer/mathstat.pdf) 
- [Monte Carlo Methods by Kroese](https://people.smp.uq.edu.au/DirkKroese/mccourse.pdf)

<br/>

{% include chapter.html name="Introduction" %}

Applications of such a model is amongst others data representation, generating artificial data and image super-resolution.
As we know from a standard AE an encoder tries to find the most relevant features of observed data and captures this information in a feature vector and the decoder has to reconstruct the original data. Hence, 
the architecture of a standard AE can be used for data representation. If we sample some features from a latent variable and feed that to the decoder we could use the decoder  for generating artificial data.
Those characteristics make an AE a useful basic structure for these goals.

{% include image.html url="/images/2023-7-6-VAE/vae_arch.png" text="An explanatory illustration that shows the model design." %}

<br/>

In a VAE, the encoder's task is to describe the distributions of those features based on the data we observed. Whereas during inference, just the decoder will be used as a generator, during training, 
the entire model will be treated as a generator model. From that perspective, sampled values from the latent variable are generated from an unknown prior distribution. Hence, we would like to construct a model based on that perspective.

*The goal of this model is to find out from which distribution family these latent variables are drawn. In addition, we would like to increase the likelihood that such a latent distribution originated from our observed data.*

Let's first formulate that mathematically:

![_config.yml]({{ site.baseurl }}/images/2023-7-6-VAE/vae_v1.png)

As a directed graph we could view it in the following way: (I made the arc representing p(x) dotted because we will later see that it is intractible.)

Applying this formulation on our model we obtain the following assignment:

![_config.yml]({{ site.baseurl }}/images/2023-7-6-VAE/vae_v1_1.png)

To visualize that concept I created an illustration:

{% include image.html url="/images/2023-7-6-VAE/diag1.png" text="This diagram illustrates assignments of distributions." %}

<br/>

![_config.yml]({{ site.baseurl }}/images/separator.png)

<br/>

{% include chapter.html name="MLE, MAP estimation and EM" %}

<br/>

A typical approach would be to use Maximum Likelihood Estimation (MLE) or Maximum A Posterior Estimation (MAP) to infer the best parameter for our model.

![_config.yml]({{ site.baseurl }}/images/2023-7-6-VAE/vae_v2.png)

![_config.yml]({{ site.baseurl }}/images/2023-7-6-VAE/vae_v3.png)

<br/>

![_config.yml]({{ site.baseurl }}/images/separator.png)

<br/>

{% include chapter.html name="Bayesian inference" %}

<br/>

Next, we want to find the posterior ``p(z|x)```.  Bayesian inference is a method in which Baye's theorem is used to update the probability for a hypothesis as more evidence becomes available. Here, we view z as the hypothesis.

![_config.yml]({{ site.baseurl }}/images/2023-7-6-VAE/vae_v5.png)

So computing the true posterior requires approximate inference but methods like Monte Carlo EM are not useful for our case since it will be too slow for complex models with large datasets.

<br/>

![_config.yml]({{ site.baseurl }}/images/separator.png)

<br/>

{% include chapter.html name="Variational inference" %}

<br/>

Instead, variational inference will bring us closer to the solution. It is a method that approximates probability density functions through optimization. The basic approach is to create a family of possible densities and with a distance measure to find the density from this family that is the closest to the one we want to approximate.

![_config.yml]({{ site.baseurl }}/images/2023-7-6-VAE/vae_v6.png)

<br/>

Equation (8) is by itself not computable in our case because it involves calculating log(p(x)). The following calculation will show that:

![_config.yml]({{ site.baseurl }}/images/2023-7-6-VAE/vae_v7.png)

As we can see in equation (6), that optimization problem contains the term p(x) which makes equation (8) not computable.

<br/>

![_config.yml]({{ site.baseurl }}/images/separator.png)

<br/>

{% include chapter.html name="The Evidence Lower Bound" %}

<br/>

Even though equation (8) is not computable we can use it after applying some tweaks.

![_config.yml]({{ site.baseurl }}/images/2023-7-6-VAE/vae_v8.png)

With some further transformations we can rewrite that function to obtain our objective:

![_config.yml]({{ site.baseurl }}/images/2023-7-6-VAE/vae_v9.png)

The name originates from the following property:

![_config.yml]({{ site.baseurl }}/images/2023-7-6-VAE/vae_v10.png)

Below you can find an animated example where p is a normal density function and q a gamma density function. 
The plot on top is the graph of the ELBO and below a plot of p and q with varying values for its parameters.
Taking the derivative of the ELBO w.r.t. to the parameter of q we see that zero is the maximum value. 

<br/> 

![_config.yml]({{ site.baseurl }}/images/2023-7-6-VAE/animation.gif)

![_config.yml]({{ site.baseurl }}/images/separator.png)

<br/>

